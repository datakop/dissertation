\chapter{Теоретические основы}

\section{Введение}

Литература \cite{шлезингер2004десять, bishop2006pattern, graves2013generating, graves2012supervised, murphy2012machine}

\subsection{Генеративные и дискриминативные методы}

\cite{graves2013generating}

\subsection{Распознавание последовательностей}

\cite{graves2013generating}

\subsubsection{Классификация последовательности}
\subsubsection{Классификация сегмента}
\subsubsection{Непрерывная временная классификация}

\section{Марковские модели}

\cite{rabiner1986, gales2008, young2006htk, Ramage2007, Fosler-Lussier1998, plotz2009markov}

\subsection{Алгоритм распространения доверия}
\subsection{Скрытые марковские модели}

Основываясь на последовательной природе рукописного текста, одним из основных методом машинного обучения и распознавания является Скрытая Марковская Модель(Hidden Markov Model, HMM).

В распознавании с HMM главной задачей является поиск самого правдоподобного набора слов $w = (w_{1},...,w_{N})$ по набору признак-векторов $x = (x_{1},...,x_{T})$, который может быть получет с помощью скользящего окна.
Для получения оптимальной последовательности слов $w$ нужно максимизировать апостериорную вероятность $P(w|x)$ по $w$. Используя правило Байеса, получим

$$w = arg max P(w|x) = arg max \frac{P(x|w)P(w)}{P(x)}$$ по $w$ из $W$ ($W$ - множество вектор-весов).

Где $P(x|w)$ - правдоподобие, $P(w)$ - априорная вероятность, $P(x)$ - маргинальная вероятность.

Большинство систем распознавания основаны на вещественных признак-векторах и следовательно к ним применимы непрерывные-HMM. Для непрерывных-HMM правдоподобие $P(x|w)$ и $P(x)$ задается функцией плотности распределения. Для дискретных-HMM вероятности задаются с помощью векторного квантования.

Обсудим каждый компонент апостериорной вероятности $P(w|x)$ отдельно:
$P(x|w)$ - отвечает на вопрос “Как” рукописный текст был написан, основываясь на обученной модели-букв. Это самый фундаментальный вопрос, на который может ответить HMM.

Априорная вероятность $p(w)$ - продукт статистической модели языка и отвечает на вопрос “что” написано. Совмещение модели письма и модели языка было одним из важнейших направлений исследования до 2010 года, большинство современных систем используют модель н-грамм по умолчанию.

Маргинальная вероятность $P(x)$ отвечает на вопрос “на сколько хорошо” написан текст в общем для всевозможных наборов слов $w$ из $W$. Так как маргинальная вероятность не зависит от $w$, то ее можно опустить при поиске оптимального набора слов $w$, но оно нужно для нормализации апостериорной вероятности для оценки качества распознавателя. “Мерой уверенности” распознавания называют апостериорную вероятность $p(w|x)$, она используется для улучшения качества распознавания.

HMM основанная на модели символа, обученная на размеченных строках, описывает правдоподобие $p(x|w)$ того, как вектор-признаки $x = (x_{1},...,x_{T})$ может быть эффективно вычислен для набора слов $w = (w_{1},...,w_{N})$ для того, чтобы найти оптимальную $w = argmax P(x|w)$ последовательность слов, моделирующую $x$, при этом используя всевозможные $w$ из $W$. Данная модель использует только внешние характеристики рукописи, модель языка здесь не используется.
HMM Модель символа описывает два стохастических процесса.  Первый - марковская цепь 1-ого порядка, которая порождает последовательность скрытых состояний $s = (s_{1},...,s_{t}$ из конечного множества $S$ удовлетворяющая свойству маркова $$P(s_{t}| s_{1},...,s_{t-1})= P(s_{t}|s_{t-1})$$ каждое состояние зависит только от предыдущего.

\section{Искусственные нейронные сети}
\cite{tay2002offline}

\subsection{Многослойный перцептрон}
\subsection{Рекуррентные нейронные сети}
\subsection{Ограниченная машина Больцмана}
\subsection{Long Short-Term Memory}
\cite{hochreiter1997long}

\subsection{Bidirectional Long Short-Term Memory}
\subsection{Слой CTC}
\cite{graves2009novel, graves2006connectionist}


\section{Статистическая модель языка}
\subsection{N-grams}
\subsection{Критерий perplexity}
\cite{fischer2012handwriting, romero2012multimodal}

\section{Модель доверия}
\cite{fischer2012handwriting}


