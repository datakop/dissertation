\chapter{Теоретические основы}

\section{Введение}

Литература \cite{шлезингер2004десять, bishop2006pattern, graves2013generating, graves2012supervised, murphy2012machine}

\subsection{Генеративные и дискриминативные методы}

\cite{graves2013generating}

\subsection{Распознавание последовательностей}

\cite{graves2013generating}

\subsubsection{Классификация последовательности}
\subsubsection{Классификация сегмента}
\subsubsection{Непрерывная временная классификация}

\section{Марковские модели}

\cite{rabiner1986, gales2008, young2006htk, Ramage2007, Fosler-Lussier1998, plotz2009markov}

\subsection{Алгоритм распространения доверия}
\subsection{Скрытые марковские модели}

Основываясь на последовательной природе рукописного текста одним из основных методом машинного обучения и распознавания является Скрытая Марковская Модель(Hidden Markov Model, HMM).

В распознавании, основанном на HMM, основной задачей является поиск самого правдоподобного набора слов w = (w1,w2,w3,.. ,wN) по набору признак-векторов x = (x1, … xT), который может быть получет с помощью скользящего окна.
Для получения оптимальной последовательности слов w нужно максимизировать апостериорную вероятность P(w|x) по w. Используя правило Байеса, получим

w = arg max P(w|x) = arg max P(x|w)P(w)/P(x) по w из W (W - множество вектор-весов).
Где P(x|w) - правдоподобие, P(w) - априорная вероятность, P(x) - evidence.
Большинство систем распознавания основаны на вещественных признак-векторах и следовательно к ним применимы непрерывные-HMM. Для непрерывных-HMM правдоподобие P(x|w) и P(x) задается функцией плотности распределения. Для дискретных-HMM вероятности задаются с помощью векторного квантования.

Обсудим каждый компонент апостериорной вероятности P(w|x) отдельно:
P(x|w) - отвечает на вопрос “Как” рукописный текст был написан, основываясь на обученной модели-букв. Это самый фундаментальный вопрос, на который может ответить HMM.

Априорная вероятность p(w) - продукт статистической модели языка и отвечает на вопрос “что” написано. Совмещение модели письма и модели языка было одним из важнейших направлений исследования до 2010 года, большинство современных систем используют модель н-грамм по умолчанию.

Маргинальная вероятность P(x) отвечает на вопрос “на сколько хорошо” написан текст в общем для всевозможных наборов слов w из W. Так как маргинальная вероятность не зависит от w, то ее можно опустить при поиске оптимального набора слов w, но оно нужно для нормализации апостериорной вероятности для оценки качества распознавателя. “мерой уверенности” распознавания называют апостериорную вероятность p(w|x), она используется для улучшения качества распознавания.

HMM

HMM основанная на модели символа, обученная на размеченных строках, описывает правдоподобие p(x|w) того, как вектор-признаки x = (x1...xT) может быть эффективно вычислен для набора слов w=(w1...wN) для того, чтобы найти оптимальную w = argmax P(x|w) последовательность слов, моделирующую x, при этом используя всевозможные w из W. Данная модель использует только внешние характеристики рукописи, модель языка здесь не используется.
HMM Модель символа описывает два стохастических процесса.  Первый - марковская цепь 1-ого порядка, которая порождает последовательность скрытых состояний s = s1 … st из конечного множества S  удовлетворяющая свойству маркова P(st| s1 … st-1)= З(st|st-1) каждое состояние зависит только от предыдущего.

\section{Искусственные нейронные сети}
\cite{tay2002offline}

\subsection{Многослойный перцептрон}
\subsection{Рекуррентные нейронные сети}
\subsection{Ограниченная машина Больцмана}
\subsection{Long Short-Term Memory}
\cite{hochreiter1997long}

\subsection{Bidirectional Long Short-Term Memory}
\subsection{Слой CTC}
\cite{graves2009novel, graves2006connectionist}


\section{Статистическая модель языка}
\subsection{N-grams}
\subsection{Критерий perplexity}
\cite{fischer2012handwriting, romero2012multimodal}

\section{Модель доверия}
\cite{fischer2012handwriting}


