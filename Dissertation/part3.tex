\chapter{Статистический подход} \label{chapt3}

Литература \cite{шлезингер2004десять, bishop2006pattern, graves2013generating, graves2012supervised, murphy2012machine}

\section{Generative and Discriminative Methods} \label{sect3_1}

\section{Sequence Recognition} \label{sect3_2}


\subsection{Sequence Classification} \label{subsect3_2_1}
\subsection{Segment Classification Segment} \label{subsect3_2_1}
\subsection{Temporal Classification} \label{subsect3_2_1}

\section{Hidden Markov Models} \label{sect3_3}

Примечание: нужно описать разные варианты реализации вероятности эмиссий наблюдаемых состояний.

Основываясь на последовательной природе рукописного текста одним из основных методом машинного обучения и распознавания является Скрытая Марковская Модель(Hidden Markov Model, HMM).

В распознавании, основанном на HMM, основной задачей является поиск самого правдоподобного набора слов w = (w1,w2,w3,.. ,wN) по набору признак-векторов x = (x1, … xT), который может быть получет с помощью скользящего окна.
Для получения оптимальной последовательности слов w нужно максимизировать апостериорную вероятность P(w|x) по w. Используя правило Байеса, получим

w = arg max P(w|x) = arg max P(x|w)P(w)/P(x) по w из W (W - множество вектор-весов).
Где P(x|w) - правдоподобие, P(w) - априорная вероятность, P(x) - evidence.
Большинство систем распознавания основаны на вещественных признак-векторах и следовательно к ним применимы непрерывные-HMM. Для непрерывных-HMM правдоподобие P(x|w) и P(x) задается функцией плотности распределения. Для дискретных-HMM вероятности задаются с помощью векторного квантования.

Обсудим каждый компонент апостериорной вероятности P(w|x) отдельно:
P(x|w) - отвечает на вопрос “Как” рукописный текст был написан, основываясь на обученной модели-букв. Это самый фундаментальный вопрос, на который может ответить HMM.

Априорная вероятность p(w) - продукт статистической модели языка и отвечает на вопрос “что” написано. Совмещение модели письма и модели языка было одним из важнейших направлений исследования до 2010 года, большинство современных систем используют модель н-грамм по умолчанию.

Маргинальная вероятность P(x) отвечает на вопрос “на сколько хорошо” написан текст в общем для всевозможных наборов слов w из W. Так как маргинальная вероятность не зависит от w, то ее можно опустить при поиске оптимального набора слов w, но оно нужно для нормализации апостериорной вероятности для оценки качества распознавателя. “мерой уверенности” распознавания называют апостериорную вероятность p(w|x), она используется для улучшения качества распознавания.

HMM

HMM основанная на модели символа, обученная на размеченных строках, описывает правдоподобие p(x|w) того, как вектор-признаки x = (x1...xT) может быть эффективно вычислен для набора слов w=(w1...wN) для того, чтобы найти оптимальную w = argmax P(x|w) последовательность слов, моделирующую x, при этом используя всевозможные w из W. Данная модель использует только внешние характеристики рукописи, модель языка здесь не используется.
HMM Модель символа описывает два стохастических процесса.  Первый - марковская цепь 1-ого порядка, которая порождает последовательность скрытых состояний s = s1 … st из конечного множества S  удовлетворяющая свойству маркова P(st| s1 … st-1)= З(st|st-1) каждое состояние зависит только от предыдущего.



\section{Artifitian Neural Networks} \label{sect3_4}

\subsection{MLP} \label{sect3_4}

\subsection{RNN} \label{sect3_4}

\subsection{LSTM} \label{sect3_4_1}

\subsection{BLSTM} \label{sect3_4_2}
